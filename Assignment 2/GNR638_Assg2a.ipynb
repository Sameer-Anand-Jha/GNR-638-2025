import pickle
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from itertools import product 

# Load train features
with open("train_image_feats_400.pkl", 'rb') as f:
    X_train = pickle.load(f)  # Shape should be (num_train_samples, 400)

# Load test features
with open("test_image_feats_400.pkl", 'rb') as f:
    X_test = pickle.load(f)  # Shape should be (num_test_samples, 400)

# Load val features
with open("val_image_feats_400.pkl", 'rb') as f:
    X_val = pickle.load(f)  # Shape should be (num_test_samples, 400)

num_classes = 21
num_train_samples = X_train.shape[0]
num_test_samples = X_test.shape[0]
num_val_samples = X_val.shape[0]

# Generate labels: Repeat each class index (0 to 20) for equal distribution
y_train = np.repeat(np.arange(num_classes), num_train_samples // num_classes)
y_test = np.repeat(np.arange(num_classes), num_test_samples // num_classes)
y_val = np.repeat(np.arange(num_classes), num_val_samples // num_classes)

# Convert to PyTorch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)


X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.long)
X_val = torch.tensor(X_val, dtype=torch.float32)
y_val = torch.tensor(y_val, dtype=torch.long)

with open("train_labels.pkl", 'wb') as f:
    pickle.dump(y_train.numpy(), f)

with open("test_labels.pkl", 'wb') as f:
    pickle.dump(y_test.numpy(), f)

with open("val_labels.pkl", 'wb') as f:
    pickle.dump(y_val.numpy(), f)






class MLP(nn.Module):
    def __init__(self, input_size=400, hidden_size1=256, hidden_size2=128, activation="relu", num_classes=21):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size1)
        self.fc2 = nn.Linear(hidden_size1, hidden_size2)
        self.fc3 = nn.Linear(hidden_size2, num_classes)

        # Set activation function
        if activation == "relu":
            self.activation = nn.ReLU()
        elif activation == "leaky_relu":
            self.activation = nn.LeakyReLU()
        elif activation == "tanh":
            self.activation = nn.Tanh()

    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.fc3(x)
        return x
'''
class MLP(nn.Module):
    def __init__(self, input_size=400, hidden_size1=512, hidden_size2=256, num_classes=21):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size1)
        self.relu1 = nn.LeakyReLU()
        self.fc2 = nn.Linear(hidden_size1, hidden_size2)
        self.relu2 = nn.LeakyReLU()
        self.fc3 = nn.Linear(hidden_size2, num_classes)
        self.dropout = nn.Dropout(p=0.3) 

    def forward(self, x):
        x = self.relu1(self.fc1(x))
        x = self.dropout(x)
        x = self.relu2(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x


# Initialize model, loss, optimizer
model = MLP()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 150


batch_size = 32
train_dataset = torch.utils.data.TensorDataset(X_train, y_train)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

for epoch in range(num_epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")


with torch.no_grad():
    outputs = model(X_test)
    predictions = torch.argmax(outputs, dim=1)
    accuracy = (predictions == y_test).float().mean()
    print(f"Test Accuracy: {accuracy:.4f}")
'''
def train_model(X_train, y_train, X_val, y_val, hidden_size1, hidden_size2, activation, num_epochs=50, batch_size=32):
    model = MLP(input_size=400, hidden_size1=hidden_size1, hidden_size2=hidden_size2, activation=activation, num_classes=21)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Prepare dataloader
    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    for epoch in range(num_epochs):
        model.train()
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

    # Evaluate on validation set
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val)
        val_predictions = torch.argmax(val_outputs, dim=1)
        val_accuracy = (val_predictions == y_val).float().mean().item()

    return val_accuracy, model

#Hyperparameter Search Function
def hyperparameter_search():
    hidden_sizes1 = [128, 256, 512]  # First hidden layer size
    hidden_sizes2 = [64, 128, 256]   # Second hidden layer size
    activations = ["relu", "leaky_relu"]
    num_epochs_list = [50,100,150,200]
    batch_size = [32, 64]

    # Generate all possible (hidden_size1, hidden_size2, activation, num_epochs) combinations
    hyperparameter_combinations = list(product(hidden_sizes1, hidden_sizes2, activations, num_epochs_list,batch_size))

    best_accuracy = 0
    best_params = None
    best_model = None

    for hidden_size1, hidden_size2, activation, num_epochs,batch_size in hyperparameter_combinations:
        print(f" Testing: Hidden Size1={hidden_size1}, Hidden Size2={hidden_size2}, Activation={activation}, Epochs={num_epochs}, Batch Size ={batch_size}")
        val_accuracy, model = train_model(X_train, y_train, X_val, y_val, hidden_size1, hidden_size2, activation, num_epochs,batch_size)

        print(f" Validation Accuracy: {val_accuracy:.4f}\n")

        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            best_params = (hidden_size1, hidden_size2, activation, num_epochs,batch_size)
            best_model = model

    print("\n Best Hyperparameters:")
    print(f"âž¡ Hidden Size1: {best_params[0]}, Hidden Size2: {best_params[1]}, Activation: {best_params[2]}, Epochs: {best_params[3]},Batch Size: {best_params[4]}")
    print(f" Best Validation Accuracy: {best_accuracy:.4f}")

    return best_model, best_params

#Train the best model
# Run Hyperparameter Search
best_model, best_params = hyperparameter_search()

# Evaluate on Test Data
def evaluate_on_test(model, X_test, y_test):
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test)
        test_predictions = torch.argmax(test_outputs, dim=1)
        test_accuracy = (test_predictions == y_test).float().mean().item()
    print(f"\n Final Test Accuracy: {test_accuracy:.4f}")
    return test_accuracy

best_test_accuracy = evaluate_on_test(best_model, X_test, y_test)
'''
def plot_best_test_accuracy(best_test_accuracy):
    """Plots a bar chart to visualize the best test accuracy."""
    plt.figure(figsize=(8, 5))
    plt.bar(["Best Hyperparameter Test Accuracy"], [best_test_accuracy],color='green',alpha=0.7, edgecolor='black')
    plt.xlabel("Best Hyperparameter Configuration")
    plt.ylabel("Test Accuracy")
    plt.title("Best Hyperparameter Test Accuracy")
    plt.ylim(0, 1)  # Ensure scale remains between 0 and 1 for accuracy visualization
    plt.grid(axis="y", linestyle="--", alpha=0.7)
    plt.savefig("Confusion_matrix.png")
    plt.show()

plot_best_test_accuracy(best_test_accuracy)
'''
def evaluate_category_wise_accuracy(model, X_test, y_test, num_classes=21):
    """Evaluates and prints accuracy for each category separately."""
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test)
        test_predictions = torch.argmax(test_outputs, dim=1)

    # Initialize counts
    category_correct = np.zeros(num_classes)
    category_total = np.zeros(num_classes)

    # Compute per-category accuracy
    for i in range(len(y_test)):
        label = y_test[i].item()
        pred = test_predictions[i].item()
        category_total[label] += 1
        if pred == label:
            category_correct[label] += 1

    # Compute and print accuracy per category
    category_accuracies = {}
    for category in range(num_classes):
        if category_total[category] > 0:
            accuracy = category_correct[category] / category_total[category]
        else:
            accuracy = 0.0  # Avoid division by zero
        category_accuracies[category] = accuracy
        print(f"Category {category}: Accuracy = {accuracy:.4f}")

    return category_accuracies

# Run category-wise accuracy evaluation
category_wise_accuracies = evaluate_category_wise_accuracy(best_model, X_test, y_test)
def plot_confusion_matrix(model, X_test, y_test, num_classes=21):
    """Generates and plots the confusion matrix."""
    model.eval()
    with torch.no_grad():
        test_outputs = model(X_test)
        test_predictions = torch.argmax(test_outputs, dim=1)

    # Compute confusion matrix
    cm = confusion_matrix(y_test.numpy(), test_predictions.numpy(), labels=np.arange(num_classes))

    # Plot confusion matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=np.arange(num_classes), yticklabels=np.arange(num_classes))
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.title("Confusion Matrix")
    plt.savefig("confusion_matrix.png")  # Save as an image file
    plt.show()

# Run confusion matrix plotting
plot_confusion_matrix(best_model, X_test, y_test)